{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/Bing/Documents/DS/DrivenData/Pover-T/Scripts/\") # need to add path to the parent folder where CV.py is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PoverTHelperTools import *\n",
    "from PoverTCV import *\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hhold_a_train, hhold_b_train, hhold_c_train = load_hhold_train()\n",
    "hhold_a_test, hhold_b_test, hhold_c_test = load_hhold_test()\n",
    "\n",
    "indiv_a_train, indiv_b_train, indiv_c_train = load_indiv_train()\n",
    "indiv_a_test, indiv_b_test, indiv_c_test = load_indiv_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhold_b_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtract mean and divide by std\n",
    "    df[numeric.columns] = (numeric- numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pre_process_data(df, enforce_cols = None):\n",
    "    \"\"\"\n",
    "    Standardize the numeric columns and one hot encode the categorical columns\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Input shape:\\t{}'.format(df.shape))\n",
    "    \n",
    "    df = standardize(df)\n",
    "    print('After standardization {}'.format(df.shape))\n",
    "    \n",
    "    # get dummies for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    print('After converting categoricals:\\t{}'.format(df.shape))\n",
    "    df.info()\n",
    "    \n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    \n",
    "    # just fill NaN with 0 for now. Later try fill it with mean or explore more to figure out how to best impute\n",
    "    df.fillna(0, inplace = True) \n",
    "    \n",
    "    \n",
    "    return df    \n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb_params = {'objective': 'binary', \n",
    "          'learning_rate': '0.1',\n",
    "          'num_threads':'4'\n",
    "          }\n",
    "\n",
    "def train_lgb(X,y):\n",
    "#     print('Training lgb model...')\n",
    "    dstrain = lgb.Dataset(X, label = y)\n",
    "#     model = lgb.train(params, dstrain, valid_sets = dstrain,\n",
    "#                       num_boost_round = 8000, verbose_eval = True)\n",
    "    model = lgb.LGBMClassifier(n_estimators = 10,\n",
    "                              objective = 'binary', \n",
    "                              num_threads = 4,\n",
    "                              learning_rate = 0.05)\n",
    "#     model.fit(X,y)\n",
    "    \n",
    "#     print('Done training!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hhold_c_train.drop(['poor', 'country'], axis = 1)\n",
    "X = pre_process_data(X)\n",
    "y = hhold_c_train['poor'].values.astype(int)\n",
    "\n",
    "n = 3\n",
    "skf = StratifiedKFold(n_splits=n)\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X,y)):\n",
    "#     print(i, train_index, val_index)\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Models here\n",
    "    \n",
    "    print('*********\\nTraining model on Fold {}'.format(i))\n",
    "    model = train_lgb(X_train, y_train)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds = model.predict_proba(X_val)\n",
    "    logloss = log_loss(y_true = y_val, y_pred= preds)\n",
    "    print('\\nLog Loss for Fold {} : {}\\n'.format(i,logloss))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lgb(X,y):\n",
    "#     print('Training lgb model...')\n",
    "    dstrain = lgb.Dataset(X, label = y)\n",
    "#     model = lgb.train(params, dstrain, valid_sets = dstrain,\n",
    "#                       num_boost_round = 8000, verbose_eval = True)\n",
    "    model = lgb.LGBMClassifier(n_estimators = 10,\n",
    "                              objective = 'binary', \n",
    "                              num_threads = 4,\n",
    "                              learning_rate = 0.05)\n",
    "#     model.fit(X,y)\n",
    "    \n",
    "#     print('Done training!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhold_a_train['poor'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(8203, 343)\n",
      "After standardization (8203, 343)\n",
      "After converting categoricals:\t(8203, 858)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8203 entries, 46107 to 39832\n",
      "Columns: 858 entries, nEsgxvAq to AlDbXTlZ_cecIq\n",
      "dtypes: float64(4), uint8(854)\n",
      "memory usage: 7.0 MB\n",
      "Training model on Fold 0\n",
      "\n",
      "Train Log Loss for Fold 0: 0.30031393913071797\n",
      "Validation Log Loss for Fold 0: 0.3419795520828201\n",
      "\n",
      "Training model on Fold 1\n",
      "\n",
      "Train Log Loss for Fold 1: 0.2992099251190147\n",
      "Validation Log Loss for Fold 1: 0.3466854565389969\n",
      "\n",
      "Training model on Fold 2\n",
      "\n",
      "Train Log Loss for Fold 2: 0.2958701127076676\n",
      "Validation Log Loss for Fold 2: 0.35914346374978195\n",
      "\n",
      "Training model on Fold 3\n",
      "\n",
      "Train Log Loss for Fold 3: 0.30015379419319776\n",
      "Validation Log Loss for Fold 3: 0.346151772180263\n",
      "\n",
      "Training model on Fold 4\n",
      "\n",
      "Train Log Loss for Fold 4: 0.2980108820786507\n",
      "Validation Log Loss for Fold 4: 0.3579809284676522\n",
      "\n",
      "Input shape:\t(3255, 440)\n",
      "After standardization (3255, 440)\n",
      "After converting categoricals:\t(3255, 1431)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3255 entries, 57071 to 4923\n",
      "Columns: 1431 entries, wJthinfa to ChbSWYhO_zmaYX\n",
      "dtypes: float64(23), uint8(1408)\n",
      "memory usage: 5.0 MB\n",
      "Training model on Fold 0\n",
      "\n",
      "Train Log Loss for Fold 0: 0.14765928972883366\n",
      "Validation Log Loss for Fold 0: 0.24426556686121204\n",
      "\n",
      "Training model on Fold 1\n",
      "\n",
      "Train Log Loss for Fold 1: 0.15224384595021023\n",
      "Validation Log Loss for Fold 1: 0.23620319088507533\n",
      "\n",
      "Training model on Fold 2\n",
      "\n",
      "Train Log Loss for Fold 2: 0.15055639321654926\n",
      "Validation Log Loss for Fold 2: 0.23603796169516766\n",
      "\n",
      "Training model on Fold 3\n",
      "\n",
      "Train Log Loss for Fold 3: 0.14854522901257014\n",
      "Validation Log Loss for Fold 3: 0.2352408157841726\n",
      "\n",
      "Training model on Fold 4\n",
      "\n",
      "Train Log Loss for Fold 4: 0.1509941603097922\n",
      "Validation Log Loss for Fold 4: 0.23121876917248432\n",
      "\n",
      "Input shape:\t(6469, 162)\n",
      "After standardization (6469, 162)\n",
      "After converting categoricals:\t(6469, 794)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6469 entries, 57211 to 7646\n",
      "Columns: 794 entries, LhUIIEHQ to eqJPmiPb_wnPqZ\n",
      "dtypes: float64(30), uint8(764)\n",
      "memory usage: 6.2 MB\n",
      "Training model on Fold 0\n",
      "\n",
      "Train Log Loss for Fold 0: 0.054156543553295744\n",
      "Validation Log Loss for Fold 0: 0.05519773150077995\n",
      "\n",
      "Training model on Fold 1\n",
      "\n",
      "Train Log Loss for Fold 1: 0.05307865863668234\n",
      "Validation Log Loss for Fold 1: 0.04918990964980026\n",
      "\n",
      "Training model on Fold 2\n",
      "\n",
      "Train Log Loss for Fold 2: 0.05168507437994638\n",
      "Validation Log Loss for Fold 2: 0.0600850094867512\n",
      "\n",
      "Training model on Fold 3\n",
      "\n",
      "Train Log Loss for Fold 3: 0.052728988601139136\n",
      "Validation Log Loss for Fold 3: 0.05906557190371984\n",
      "\n",
      "Training model on Fold 4\n",
      "\n",
      "Train Log Loss for Fold 4: 0.051123761933806\n",
      "Validation Log Loss for Fold 4: 0.060991860079799544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## using 50 estimators, submitted to LB and got 0.46511\n",
    "## this was improvement over 10 estimators which was 0.51925\n",
    "LGB = lgb.LGBMClassifier(n_estimators = 50,\n",
    "                              objective = 'binary', \n",
    "                              num_threads = 4,\n",
    "                              learning_rate = 0.05)\n",
    "StratifiedKF(hhold_a_train, n_splits = 5, model = LGB)\n",
    "StratifiedKF(hhold_b_train, n_splits = 5, model = LGB)\n",
    "StratifiedKF(hhold_c_train, n_splits = 5, model = LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(8203, 343)\n",
      "After standardization (8203, 343)\n",
      "After converting categoricals:\t(8203, 858)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8203 entries, 46107 to 39832\n",
      "Columns: 858 entries, nEsgxvAq to AlDbXTlZ_cecIq\n",
      "dtypes: float64(4), uint8(854)\n",
      "memory usage: 7.0 MB\n",
      "Training model on Fold 0\n",
      "\n",
      "Train Log Loss for Fold 0: 0.20732952243816363\n",
      "Validation Log Loss for Fold 0: 0.2951030977329212\n",
      "\n",
      "Training model on Fold 1\n",
      "\n",
      "Train Log Loss for Fold 1: 0.2076755199597848\n",
      "Validation Log Loss for Fold 1: 0.2950823640716657\n",
      "\n",
      "Training model on Fold 2\n",
      "\n",
      "Train Log Loss for Fold 2: 0.2054084064077528\n",
      "Validation Log Loss for Fold 2: 0.3033640988838772\n",
      "\n",
      "Training model on Fold 3\n",
      "\n",
      "Train Log Loss for Fold 3: 0.20859026080339288\n",
      "Validation Log Loss for Fold 3: 0.29284094890326373\n",
      "\n",
      "Training model on Fold 4\n",
      "\n",
      "Train Log Loss for Fold 4: 0.20664344989476197\n",
      "Validation Log Loss for Fold 4: 0.30450283977840614\n",
      "\n",
      "Input shape:\t(3255, 440)\n",
      "After standardization (3255, 440)\n",
      "After converting categoricals:\t(3255, 1431)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3255 entries, 57071 to 4923\n",
      "Columns: 1431 entries, wJthinfa to ChbSWYhO_zmaYX\n",
      "dtypes: float64(23), uint8(1408)\n",
      "memory usage: 5.0 MB\n",
      "Training model on Fold 0\n",
      "\n",
      "Train Log Loss for Fold 0: 0.07642369313939708\n",
      "Validation Log Loss for Fold 0: 0.23384402243208066\n",
      "\n",
      "Training model on Fold 1\n",
      "\n",
      "Train Log Loss for Fold 1: 0.07975373597680278\n",
      "Validation Log Loss for Fold 1: 0.21537699267594107\n",
      "\n",
      "Training model on Fold 2\n",
      "\n",
      "Train Log Loss for Fold 2: 0.0796918025275385\n",
      "Validation Log Loss for Fold 2: 0.22226302774428194\n",
      "\n",
      "Training model on Fold 3\n",
      "\n",
      "Train Log Loss for Fold 3: 0.07770616033100712\n",
      "Validation Log Loss for Fold 3: 0.22395573939010655\n",
      "\n",
      "Training model on Fold 4\n",
      "\n",
      "Train Log Loss for Fold 4: 0.07984392069605833\n",
      "Validation Log Loss for Fold 4: 0.21127100702697738\n",
      "\n",
      "Input shape:\t(6469, 162)\n",
      "After standardization (6469, 162)\n",
      "After converting categoricals:\t(6469, 794)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6469 entries, 57211 to 7646\n",
      "Columns: 794 entries, LhUIIEHQ to eqJPmiPb_wnPqZ\n",
      "dtypes: float64(30), uint8(764)\n",
      "memory usage: 6.2 MB\n",
      "Training model on Fold 0\n",
      "\n",
      "Train Log Loss for Fold 0: 0.014594389127977113\n",
      "Validation Log Loss for Fold 0: 0.019637647394389633\n",
      "\n",
      "Training model on Fold 1\n",
      "\n",
      "Train Log Loss for Fold 1: 0.01465894997151347\n",
      "Validation Log Loss for Fold 1: 0.0109632622670388\n",
      "\n",
      "Training model on Fold 2\n",
      "\n",
      "Train Log Loss for Fold 2: 0.013239409105515404\n",
      "Validation Log Loss for Fold 2: 0.027638736416066897\n",
      "\n",
      "Training model on Fold 3\n",
      "\n",
      "Train Log Loss for Fold 3: 0.013961342023332742\n",
      "Validation Log Loss for Fold 3: 0.02482648752699376\n",
      "\n",
      "Training model on Fold 4\n",
      "\n",
      "Train Log Loss for Fold 4: 0.012528938125685687\n",
      "Validation Log Loss for Fold 4: 0.027869102904021035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## using 50 estimators, submitted to LB and got 0.46511\n",
    "## this was improvement over 10 estimators which was 0.51925\n",
    "LGB = lgb.LGBMClassifier(n_estimators = 100,\n",
    "                              objective = 'binary', \n",
    "                              num_threads = 4,\n",
    "                              learning_rate = 0.05\n",
    "                              )\n",
    "StratifiedKF(hhold_a_train, n_splits = 5, model = LGB)\n",
    "StratifiedKF(hhold_b_train, n_splits = 5, model = LGB)\n",
    "StratifiedKF(hhold_c_train, n_splits = 5, model = LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(8203, 343)\n",
      "After standardization (8203, 343)\n",
      "After converting categoricals:\t(8203, 858)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8203 entries, 46107 to 39832\n",
      "Columns: 858 entries, nEsgxvAq to AlDbXTlZ_cecIq\n",
      "dtypes: float64(4), uint8(854)\n",
      "memory usage: 7.0 MB\n",
      "Training model on Fold 0\n",
      "\n",
      "Train Log Loss for Fold 0: 0.22823990017457374\n",
      "Validation Log Loss for Fold 0: 0.3059116912596034\n",
      "\n",
      "Training model on Fold 1\n",
      "\n",
      "Train Log Loss for Fold 1: 0.23005498987206427\n",
      "Validation Log Loss for Fold 1: 0.3057219540590821\n",
      "\n",
      "Training model on Fold 2\n",
      "\n",
      "Train Log Loss for Fold 2: 0.23532374467478295\n",
      "Validation Log Loss for Fold 2: 0.27715063779644333\n",
      "\n",
      "Training model on Fold 3\n",
      "\n",
      "Train Log Loss for Fold 3: 0.23115459640498226\n",
      "Validation Log Loss for Fold 3: 0.2995294605843794\n",
      "\n",
      "Training model on Fold 4\n",
      "\n",
      "Train Log Loss for Fold 4: 0.22802966936431865\n",
      "Validation Log Loss for Fold 4: 0.31777206866535707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "StratifiedKF(hhold_a_train, n_splits = 5, model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StratifiedKF(hhold_b_train, n_splits = 5, model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "StratifiedKF(hhold_c_train, n_splits = 10, model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The log loss on the leaderboard for Logistic Regression is 3.2 but on my CV here it is like 0.18\n",
    "### what's going on???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [DeepLearning]",
   "language": "python",
   "name": "Python [DeepLearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
